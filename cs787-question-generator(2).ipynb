{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanush/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Please use python 3.9 environment\n",
    "# Working version link (in case this notebook is not running properly): https://www.kaggle.com/code/aathanush/cs787-question-generator/edit \n",
    "#Install these libraries if you haven't already (uncomment the line below to install them)\n",
    "# !pip install -U transformers accelerate datasets huggingface_hub -q\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this in case of out of memory errors. Else, no need to run this code\n",
    "#!pip install GPUtil\n",
    "#For clearing GPU cache\n",
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "    torch.cuda.empty_cache()\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "\n",
    "free_gpu_cache()                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1: Loading and Preprocessing Data ---\n",
      "Successfully loaded dataset. Initial shape: (120406, 12)\n",
      "Shape after filtering for grades [10, 11, 12]: (98199, 12)\n",
      "Shape after filtering for subjects ['Biology', 'Chemistry', 'Physics', 'Science']: (30706, 12)\n",
      "Shape after dropping null values: (30706, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 1: Loading and Preprocessing Data ---\")\n",
    "# Requires internet connection as we are downloading data form huggingface\n",
    "# Load the dataset from the Hugging Face Hub\n",
    "try:\n",
    "    df = pd.read_csv(\"hf://datasets/KadamParth/Ncert_dataset/NCERT_Dataset.csv\")\n",
    "    print(f\"Successfully loaded dataset. Initial shape: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "\n",
    "\n",
    "# Filter for grades 10, 11, and 12\n",
    "df = df[df['grade'].isin([10, 11, 12])]\n",
    "print(f\"Shape after filtering for grades [10, 11, 12]: {df.shape}\")\n",
    "\n",
    "# Filter for the specified subjects\n",
    "allowed_subjects = [\"Biology\", \"Chemistry\", \"Physics\", \"Science\"]\n",
    "df = df[df['subject'].isin(allowed_subjects)]\n",
    "print(f\"Shape after filtering for subjects {allowed_subjects}: {df.shape}\")\n",
    "\n",
    "# Drop rows with any null values (especially in 'Answer')\n",
    "df.dropna(inplace=True)\n",
    "print(f\"Shape after dropping null values: {df.shape}\")\n",
    "\n",
    "# Reset index after filtering\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "df['qg_input_text'] = df.apply(\n",
    "    lambda row: f\"Generate a {row['Difficulty']} question for a grade {row['grade']} {row['subject']} student using this context: {row['Explanation']}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df['ag_input_text'] = df.apply(\n",
    "    lambda row: f\"Answer the following {row['Difficulty']} grade {row['grade']} {row['subject']} question. Explanation: {row['Explanation']} Question: {row['Question']}\",\n",
    "    axis=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TRAINING \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, input_col, target_col, model_output_dir):\n",
    "    \"\"\"\n",
    "    Trains a T5 model on the provided dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The preprocessed dataframe.\n",
    "        input_col (str): The name of the column containing the input text.\n",
    "        target_col (str): The name of the column containing the target text.\n",
    "        model_output_dir (str): The directory to save the trained model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Training for: {model_output_dir} ---\")\n",
    "\n",
    "    model_df = df[[input_col, target_col]].copy()\n",
    "    model_df.rename(columns={input_col: 'input_text', target_col: 'target_text'}, inplace=True)\n",
    "\n",
    "    dataset = Dataset.from_pandas(model_df)\n",
    "\n",
    "    train_test_split_dataset = dataset.train_test_split(test_size=0.1)\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': train_test_split_dataset['train'],\n",
    "        'validation': train_test_split_dataset['test']\n",
    "    })\n",
    "    print(f\"Dataset prepared and split:\\n{dataset_dict}\")\n",
    "\n",
    "    # MODEL_NAME = 't5-small'\n",
    "    MODEL_NAME = 't5-base'\n",
    "    #MODEL_NAME = \"t5-large\" t5-large was not working due to large size and lower compute hence its results were not published in report\n",
    "    tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    MAX_INPUT_LENGTH = 512  \n",
    "    MAX_TARGET_LENGTH = 128 \n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        model_inputs = tokenizer(\n",
    "            examples['input_text'],\n",
    "            max_length=MAX_INPUT_LENGTH,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "        labels = tokenizer(\n",
    "            text_target=examples['target_text'],\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "        model_inputs['labels'] = labels['input_ids']\n",
    "        return model_inputs\n",
    "\n",
    "    tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
    "    print(\"Tokenization complete.\")\n",
    "\n",
    "    model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=model_output_dir,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy='steps',\n",
    "    eval_strategy='steps',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f'./logs/{model_output_dir}',\n",
    "    logging_steps=500, \n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets['train'],\n",
    "        eval_dataset=tokenized_datasets['validation'],\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    print(f\"Starting training for {model_output_dir}. This might take a while...\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(f\"Training finished. Saving model to {model_output_dir}\")\n",
    "    trainer.save_model(model_output_dir)\n",
    "    tokenizer.save_pretrained(model_output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T02:39:02.001983Z",
     "iopub.status.busy": "2025-11-13T02:39:02.001633Z",
     "iopub.status.idle": "2025-11-13T09:30:16.778873Z",
     "shell.execute_reply": "2025-11-13T09:30:16.777981Z",
     "shell.execute_reply.started": "2025-11-13T02:39:02.001958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training for: ./question_generator_model_t5base ---\n",
      "Dataset prepared and split:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'target_text'],\n",
      "        num_rows: 27635\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_text', 'target_text'],\n",
      "        num_rows: 3071\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221537c128f2446288e8bad495d73b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571bbca21c1840078a71d87851bb4cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f909912c4d4cc9ac2fec929007dd03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8242954e7049d19d948866695edb94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27635 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9459f13f364ad7a9556a8ed52f13fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3071 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f640c681057c4358a7faa8e0cd739171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2418d9b445e7440b821dfe80b0134205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48/2673656847.py:84: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for ./question_generator_model_t5base. This might take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10365' max='10365' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10365/10365 3:24:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.590200</td>\n",
       "      <td>0.217149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.214900</td>\n",
       "      <td>0.199870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.207400</td>\n",
       "      <td>0.186916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.195400</td>\n",
       "      <td>0.181906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.190800</td>\n",
       "      <td>0.176095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.189300</td>\n",
       "      <td>0.172204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.170081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.167600</td>\n",
       "      <td>0.167327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.165953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.168500</td>\n",
       "      <td>0.162993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.161896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.167500</td>\n",
       "      <td>0.160440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.167000</td>\n",
       "      <td>0.159028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.164200</td>\n",
       "      <td>0.158622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.153600</td>\n",
       "      <td>0.157962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.153300</td>\n",
       "      <td>0.156951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.150400</td>\n",
       "      <td>0.156777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.154900</td>\n",
       "      <td>0.155869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.152600</td>\n",
       "      <td>0.155348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.149600</td>\n",
       "      <td>0.155325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished. Saving model to ./question_generator_model_t5base\n",
      "\n",
      "--- Starting Training for: ./answer_generator_model_t5base ---\n",
      "Dataset prepared and split:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'target_text'],\n",
      "        num_rows: 27635\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_text', 'target_text'],\n",
      "        num_rows: 3071\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc07fb5ab76481aa7ef7cd75ee2726d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27635 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f77dbf836a341eea72948d54a334783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3071 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48/2673656847.py:84: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for ./answer_generator_model_t5base. This might take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10365' max='10365' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10365/10365 3:25:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.614800</td>\n",
       "      <td>0.577911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.622600</td>\n",
       "      <td>0.542167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.602000</td>\n",
       "      <td>0.521803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.573900</td>\n",
       "      <td>0.509029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.556700</td>\n",
       "      <td>0.499482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.550300</td>\n",
       "      <td>0.491342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.545900</td>\n",
       "      <td>0.486965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.525300</td>\n",
       "      <td>0.482715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.515300</td>\n",
       "      <td>0.478218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.514600</td>\n",
       "      <td>0.474124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.511600</td>\n",
       "      <td>0.469302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.503900</td>\n",
       "      <td>0.467717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.512900</td>\n",
       "      <td>0.464361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.496200</td>\n",
       "      <td>0.463305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.478600</td>\n",
       "      <td>0.460222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.489800</td>\n",
       "      <td>0.458549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.483500</td>\n",
       "      <td>0.458118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.481800</td>\n",
       "      <td>0.456725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.483400</td>\n",
       "      <td>0.456370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.472800</td>\n",
       "      <td>0.455532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished. Saving model to ./answer_generator_model_t5base\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Train Model 1: Question Generator ---\n",
    "train_model(\n",
    "    df=df,\n",
    "    input_col='qg_input_text',\n",
    "    target_col='Question',\n",
    "    model_output_dir='./question_generator_model_t5base'\n",
    ")\n",
    "\n",
    "# --- Train Model 2: Answer Generator ---\n",
    "train_model(\n",
    "    df=df, \n",
    "    input_col='ag_input_text',\n",
    "    target_col='Answer',\n",
    "    model_output_dir='./answer_generator_model_t5base'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:41:05.787990Z",
     "iopub.status.busy": "2025-11-13T09:41:05.787270Z",
     "iopub.status.idle": "2025-11-13T09:41:13.875074Z",
     "shell.execute_reply": "2025-11-13T09:41:13.874417Z",
     "shell.execute_reply.started": "2025-11-13T09:41:05.787956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Step 4: Inference Example ---\n",
      "Models loaded successfully.\n",
      "\n",
      "--- Generating a new question based on this context: ---\n",
      "Subject: Chemistry, Grade: 11, Difficulty: Medium\n",
      "Explanation: In the periodic table, the electronic configuration of elements determines various properties, including atomic radii. For Group 1 elements (alkali metals like Na, K, Rb, Cs), the atomic radius increases as you move down the group. This is because each subsequent element has an additional electron shell, which increases the distance from the nucleus to the outermost electrons. In contrast, for Group 17 elements (halogens like F, Cl, Br, I), the atomic radius decreases as you move down the group....\n",
      "\n",
      "GENERATED QUESTION:\n",
      "Describe the relationship between the number of electron shells and the atomic radius for Group 1 elements.\n",
      "\n",
      "ORIGINAL QUESTION (for comparison):\n",
      "Describe the general outer electronic configuration of p-block elements.\n",
      "\n",
      "GENERATED ANSWER (for the new question):\n",
      "The atomic radius for Group 1 elements increases as you move down the group. This is because each subsequent element has an additional electron shell, which increases the distance from the nucleus to the outermost electrons.\n",
      "\n",
      "ORIGINAL ANSWER (for comparison):\n",
      "The general outer electronic configuration of p-block elements is ns^2 np^1-6, where n is the principal quantum number.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\\n--- Step 4: Inference Example ---\")\n",
    "qg_model_path = './question_generator_model_t5base'\n",
    "ag_model_path = './answer_generator_model_t5base'\n",
    "\n",
    "qg_tokenizer = T5Tokenizer.from_pretrained(qg_model_path)\n",
    "qg_model = T5ForConditionalGeneration.from_pretrained(qg_model_path).to(device)\n",
    "\n",
    "ag_tokenizer = T5Tokenizer.from_pretrained(ag_model_path)\n",
    "ag_model = T5ForConditionalGeneration.from_pretrained(ag_model_path).to(device)\n",
    "\n",
    "print(\"Models loaded successfully.\")\n",
    "\n",
    "# --- Create generation functions ---\n",
    "def generate_question(explanation, grade, subject, difficulty, complexity):\n",
    "    \"\"\"Generates a question using the fine-tuned QG model.\"\"\"\n",
    "    input_text = f\"Generate a {difficulty} question for a grade {grade} {subject} student using this context: {explanation}\"\n",
    "    \n",
    "    inputs = qg_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    \n",
    "    outputs = qg_model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=128,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    generated_question = qg_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_question\n",
    "\n",
    "def generate_answer(explanation, grade, subject, difficulty, question):\n",
    "    \"\"\"Generates an answer using the fine-tuned AG model.\"\"\"\n",
    "    input_text = f\"Answer the following {difficulty} grade {grade} {subject} question. Explanation: {explanation} Question: {question}\"\n",
    "    \n",
    "    inputs = ag_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    \n",
    "    outputs = ag_model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=256, # Answers can be longer\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    generated_answer = ag_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_answer\n",
    "\n",
    "\n",
    "# --- Let's test it with a sample from our dataset ---\n",
    "sample_row = df.iloc[15090] # Take a random sample to test\n",
    "\n",
    "explanation = sample_row['Explanation']\n",
    "grade = sample_row['grade']\n",
    "subject = sample_row['subject']\n",
    "difficulty = sample_row['Difficulty']\n",
    "complexity = sample_row['QuestionComplexity']\n",
    "\n",
    "print(\"\\n--- Generating a new question based on this context: ---\")\n",
    "print(f\"Subject: {subject}, Grade: {grade}, Difficulty: {difficulty}\")\n",
    "print(f\"Explanation: {explanation[:500]}...\") # Print first 200 chars\n",
    "\n",
    "# 1. Generate the question\n",
    "generated_q = generate_question(explanation, grade, subject, difficulty, complexity)\n",
    "print(\"\\nGENERATED QUESTION:\")\n",
    "print(generated_q)\n",
    "print(\"\\nORIGINAL QUESTION (for comparison):\")\n",
    "print(sample_row['Question'])\n",
    "\n",
    "# 2. Generate the answer for the newly generated question\n",
    "generated_a = generate_answer(explanation, grade, subject, difficulty, generated_q)\n",
    "print(\"\\nGENERATED ANSWER (for the new question):\")\n",
    "print(generated_a)\n",
    "print(\"\\nORIGINAL ANSWER (for comparison):\")\n",
    "print(sample_row['Answer'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:41:21.672471Z",
     "iopub.status.busy": "2025-11-13T09:41:21.671824Z",
     "iopub.status.idle": "2025-11-13T09:43:53.906394Z",
     "shell.execute_reply": "2025-11-13T09:43:53.905516Z",
     "shell.execute_reply.started": "2025-11-13T09:41:21.672449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Step 5: Evaluating Model Performance ---\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00137ba2a2b496887518fb16613ced9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e10c2dc2304d4abd263d3d32c94966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics loaded successfully.\n",
      "Running evaluation on 100 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c0090273c54c2a931e64f0dd2052db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating for Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calculating Scores for Question Generation Model ---\n",
      "BLEU Score: 28.06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85eed9e4ae2b464c83425194eaa50997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575467680091485595019c1ab3e76d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f096134701e4412d8263f24d225c53b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7291c1fbafa9480d92bd61f5c6038a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a3366d507443f9bd1e868ab108b9bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8454f6a1b2004ecfa37c8dc9695f0573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore (F1): 0.9217\n",
      "\n",
      "--- Calculating Scores for Answer Generation Model ---\n",
      "BLEU Score: 15.88\n",
      "BERTScore (F1): 0.8832\n",
      "\n",
      "--- What do these scores mean? ---\n",
      "BLEU Score: Measures word/phrase overlap. A score of 0-10 is poor, 10-20 is okay, 20-30 is good, and >30 is considered high quality. It's scaled from 0 to 100.\n",
      "BERTScore (F1): Measures semantic similarity (meaning). A higher score is better (ranges from 0 to 1). It is generally a more reliable indicator of quality than BLEU for tasks like this.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n--- Step 5: Evaluating Model Performance ---\")\n",
    "\n",
    "#!pip install evaluate sacrebleu bert-score tqdm -q\n",
    "\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Load the metric calculators\n",
    "try:\n",
    "    bleu_metric = evaluate.load('sacrebleu')\n",
    "    bertscore_metric = evaluate.load('bertscore')\n",
    "    print(\"Evaluation metrics loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load metrics: {e}\")\n",
    "\n",
    "# Let's evaluate on a random sample of 100 items from our original filtered dataframe\n",
    "# to get a good statistical measure without taking too long.\n",
    "EVAL_SAMPLE_SIZE = 100\n",
    "if len(df) > EVAL_SAMPLE_SIZE:\n",
    "    eval_df = df.sample(n=EVAL_SAMPLE_SIZE, random_state=42)\n",
    "else:\n",
    "    eval_df = df\n",
    "\n",
    "print(f\"Running evaluation on {len(eval_df)} samples...\")\n",
    "\n",
    "# Store the generated texts and the reference texts\n",
    "generated_questions = []\n",
    "reference_questions = []\n",
    "generated_answers = []\n",
    "reference_answers = []\n",
    "\n",
    "# Loop through the evaluation sample and generate predictions\n",
    "for _, row in tqdm(eval_df.iterrows(), total=len(eval_df), desc=\"Generating for Evaluation\"):\n",
    "    # --- Part 1: Evaluate the Question Generator ---\n",
    "    # Get the reference (original) question\n",
    "    ref_q = row['Question']\n",
    "    reference_questions.append(ref_q)\n",
    "    \n",
    "    # Generate a new question from the context\n",
    "    gen_q = generate_question(\n",
    "        explanation=row['Explanation'],\n",
    "        grade=row['grade'],\n",
    "        subject=row['subject'],\n",
    "        difficulty=row['Difficulty'],\n",
    "        complexity=row['QuestionComplexity']\n",
    "    )\n",
    "    generated_questions.append(gen_q)\n",
    "\n",
    "    # --- Part 2: Evaluate the Answer Generator ---\n",
    "    # Get the reference (original) answer\n",
    "    ref_a = row['Answer']\n",
    "    reference_answers.append(ref_a)\n",
    "    \n",
    "    # Generate an answer for the *newly generated question*\n",
    "    # This tests the full pipeline\n",
    "    gen_a = generate_answer(\n",
    "        explanation=row['Explanation'],\n",
    "        grade=row['grade'],\n",
    "        subject=row['subject'],\n",
    "        difficulty=row['Difficulty'],\n",
    "        question=gen_q # Use the generated question as input\n",
    "    )\n",
    "    generated_answers.append(gen_a)\n",
    "\n",
    "# --- Now, calculate the scores ---\n",
    "\n",
    "# For BLEU, the references need to be in a list of lists.\n",
    "bleu_references_q = [[q] for q in reference_questions]\n",
    "bleu_references_a = [[a] for a in reference_answers]\n",
    "\n",
    "print(\"\\n--- Calculating Scores for Question Generation Model ---\")\n",
    "try:\n",
    "    # Calculate BLEU Score for Questions\n",
    "    bleu_score_q = bleu_metric.compute(predictions=generated_questions, references=bleu_references_q)\n",
    "    print(f\"BLEU Score: {bleu_score_q['score']:.2f}\")\n",
    "\n",
    "    # Calculate BERTScore for Questions\n",
    "    bert_score_q = bertscore_metric.compute(predictions=generated_questions, references=reference_questions, lang=\"en\")\n",
    "    # We take the average F1 score\n",
    "    avg_f1_q = np.mean(bert_score_q['f1'])\n",
    "    print(f\"BERTScore (F1): {avg_f1_q:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during question scoring: {e}\")\n",
    "\n",
    "print(\"\\n--- Calculating Scores for Answer Generation Model ---\")\n",
    "try:\n",
    "    # Calculate BLEU Score for Answers\n",
    "    bleu_score_a = bleu_metric.compute(predictions=generated_answers, references=bleu_references_a)\n",
    "    print(f\"BLEU Score: {bleu_score_a['score']:.2f}\")\n",
    "\n",
    "    # Calculate BERTScore for Answers\n",
    "    bert_score_a = bertscore_metric.compute(predictions=generated_answers, references=reference_answers, lang=\"en\")\n",
    "    avg_f1_a = np.mean(bert_score_a['f1'])\n",
    "    print(f\"BERTScore (F1): {avg_f1_a:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during answer scoring: {e}\")\n",
    "\n",
    "\n",
    "# --- Explanation of Scores ---\n",
    "print(\"\\n--- What do these scores mean? ---\")\n",
    "print(\"BLEU Score: Measures word/phrase overlap. A score of 0-10 is poor, 10-20 is okay, 20-30 is good, and >30 is considered high quality. It's scaled from 0 to 100.\")\n",
    "print(\"BERTScore (F1): Measures semantic similarity (meaning). A higher score is better (ranges from 0 to 1). It is generally a more reliable indicator of quality than BLEU for tasks like this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "torch.set_num_threads(4) \n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "def train_model_bart(df, input_col, target_col, model_output_dir):\n",
    "    \"\"\"\n",
    "    Trains a BART model on the provided dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The preprocessed dataframe.\n",
    "        input_col (str): The name of the column containing the input text.\n",
    "        target_col (str): The name of the column containing the target text.\n",
    "        model_output_dir (str): The directory to save the trained model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Training for: {model_output_dir} ---\")\n",
    "\n",
    "    model_df = df[[input_col, target_col]].copy()\n",
    "    model_df.rename(columns={input_col: 'input_text', target_col: 'target_text'}, inplace=True)\n",
    "\n",
    "    dataset = Dataset.from_pandas(model_df)\n",
    "\n",
    "    train_test_split_dataset = dataset.train_test_split(test_size=0.1)\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': train_test_split_dataset['train'],\n",
    "        'validation': train_test_split_dataset['test']\n",
    "    })\n",
    "    print(f\"Dataset prepared and split:\\n{dataset_dict}\")\n",
    "\n",
    "\n",
    "    MODEL_NAME = 'facebook/bart-base'\n",
    "\n",
    "    tokenizer = BartTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    MAX_INPUT_LENGTH = 512  \n",
    "    MAX_TARGET_LENGTH = 512 \n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        model_inputs = tokenizer(\n",
    "            examples['input_text'],\n",
    "            max_length=MAX_INPUT_LENGTH,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "        labels = tokenizer(\n",
    "            text_target=examples['target_text'],\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "        model_inputs['labels'] = labels['input_ids']\n",
    "        return model_inputs\n",
    "\n",
    "    tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
    "    print(\"Tokenization complete.\")\n",
    "\n",
    "    model = BartForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_output_dir,\n",
    "        num_train_epochs=3,\n",
    "        save_strategy='steps',\n",
    "        eval_strategy='steps',\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f'./logs/{model_output_dir}',\n",
    "        logging_steps=500,\n",
    "        save_steps=500,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets['train'],\n",
    "        eval_dataset=tokenized_datasets['validation'],\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    print(f\"Starting training for {model_output_dir}. This might take a while...\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(f\"Training finished. Saving model to {model_output_dir}\")\n",
    "    trainer.save_model(model_output_dir)\n",
    "    tokenizer.save_pretrained(model_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b21a6fd6004a9782d4f97c3abaa53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/27635 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7556f72d9b497eb151ae9f3a0df53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3071 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7b/wlrj7z552xb4wm1nq29w37vr0000gn/T/ipykernel_14430/312478582.py:176: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training... (This will still be slower than GPU, but optimized)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanush/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/thanush/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/thanush/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/thanush/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='864' max='864' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [864/864 2:04:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.604800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.830700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.509100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.461800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.441200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.420800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.432600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.406200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.416500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.391700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.380400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.391800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.387900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.374600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.380700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.378300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./question_generator_model_bart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanush/Library/Python/3.9/lib/python/site-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c56938852f40bd98ef5954011b5cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/27635 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e6e2d605494f67a7f36ce509eeb6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3071 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7b/wlrj7z552xb4wm1nq29w37vr0000gn/T/ipykernel_14430/312478582.py:176: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training... (This will still be slower than GPU, but optimized)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanush/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/thanush/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/thanush/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/thanush/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='864' max='864' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [864/864 2:04:24, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.784400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.635400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.349100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.277500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.227900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.217900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.168500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.170300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.149900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.103700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.106500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.100300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.116400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.111900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.113300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./answer_generator_model_bart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanush/Library/Python/3.9/lib/python/site-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_model_bart(\n",
    "    df=df, \n",
    "    input_col='qg_input_text',\n",
    "    target_col='Question',\n",
    "    model_output_dir='./question_generator_model_bart'\n",
    ")\n",
    "\n",
    "train_model_bart(\n",
    "    df=df, \n",
    "    input_col='ag_input_text',\n",
    "    target_col='Answer',\n",
    "    model_output_dir='./answer_generator_model_bart'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Step 4: Inference Example (BART) ---\n",
      "BART models loaded successfully.\n",
      "\n",
      "--- Generating a new question based on this context (BART): ---\n",
      "Subject: Physics, Grade: 12, Difficulty: Medium\n",
      "Explanation: Electrons carry a fundamental unit of charge, which is measured in Coulombs (C). The charge of one electron is approximately 1.6 × 10^-19 C. To calculate the time required to accumulate a certain amou...\n",
      "\n",
      "GENERATED QUESTION (BART):\n",
      "Describe the relationship between current, charge, and time in an electric circuit.\n",
      "\n",
      "ORIGINAL QUESTION (for comparison):\n",
      "If 10^10 electrons move from one body to another every second, how long will it take to transfer 1 C of charge?\n",
      "\n",
      "GENERATED ANSWER (for the new question) (BART):\n",
      "To calculate the time required to accumulate a certain amount of electric charge, you need to understand the rate at which electrons are transferred. This rate is often given as current (I).\n",
      "\n",
      "ORIGINAL ANSWER (for comparison):\n",
      "The charge transferred per second is 1.6 Ã— 10^-19 C Ã— 10^10 = 1.6 Ã— 10^-9 C/s. The time required is 1 C / 1.6 Ã— 10^-9 C/s = 6.25 Ã— 10^8 s â‰ˆ 19.8 years.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n--- Step 4: Inference Example (BART) ---\")\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "qg_model_path = './question_generator_model_bart'\n",
    "ag_model_path = './answer_generator_model_bart'\n",
    "\n",
    "qg_tokenizer = BartTokenizer.from_pretrained(qg_model_path)\n",
    "qg_model = BartForConditionalGeneration.from_pretrained(qg_model_path).to(device)\n",
    "\n",
    "ag_tokenizer = BartTokenizer.from_pretrained(ag_model_path)\n",
    "ag_model = BartForConditionalGeneration.from_pretrained(ag_model_path).to(device)\n",
    "\n",
    "print(\"BART models loaded successfully.\")\n",
    "\n",
    "def generate_question(explanation, grade, subject, difficulty, complexity):\n",
    "    \"\"\"Generates a question using the fine-tuned BART QG model.\"\"\"\n",
    "    input_text = f\"Generate a {difficulty} question with complexity {complexity:.2f} for a grade {grade} {subject} student. Explanation: {explanation}\"\n",
    "    \n",
    "    inputs = qg_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    \n",
    "    outputs = qg_model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=128,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    generated_question = qg_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_question\n",
    "\n",
    "def generate_answer(explanation, grade, subject, difficulty, question):\n",
    "    \"\"\"Generates an answer using the fine-tuned BART AG model.\"\"\"\n",
    "    input_text = f\"Answer the following {difficulty} grade {grade} {subject} question. Explanation: {explanation} Question: {question}\"\n",
    "    \n",
    "    inputs = ag_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    \n",
    "    outputs = ag_model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=256, # Answers can be longer\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    generated_answer = ag_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_answer\n",
    "\n",
    "\n",
    "sample_row = df.iloc[100] # Take a random sample to test\n",
    "\n",
    "explanation = sample_row['Explanation']\n",
    "grade = sample_row['grade']\n",
    "subject = sample_row['subject']\n",
    "difficulty = sample_row['Difficulty']\n",
    "complexity = sample_row['QuestionComplexity']\n",
    "\n",
    "print(\"\\n--- Generating a new question based on this context (BART): ---\")\n",
    "print(f\"Subject: {subject}, Grade: {grade}, Difficulty: {difficulty}\")\n",
    "print(f\"Explanation: {explanation[:200]}...\") # Print first 200 chars\n",
    "\n",
    "# 1. Generate the question\n",
    "generated_q = generate_question(explanation, grade, subject, difficulty, complexity)\n",
    "print(\"\\nGENERATED QUESTION (BART):\")\n",
    "print(generated_q)\n",
    "print(\"\\nORIGINAL QUESTION (for comparison):\")\n",
    "print(sample_row['Question'])\n",
    "\n",
    "# 2. Generate the answer for the newly generated question\n",
    "generated_a = generate_answer(explanation, grade, subject, difficulty, generated_q)\n",
    "print(\"\\nGENERATED ANSWER (for the new question) (BART):\")\n",
    "print(generated_a)\n",
    "print(\"\\nORIGINAL ANSWER (for comparison):\")\n",
    "print(sample_row['Answer'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Step 5: Evaluating BART Model Performance ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7b7f8859544e85885269554a57a24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697e7d29171640d2bd646013282b8514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics loaded successfully.\n",
      "Running evaluation on 100 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481a76b4f834416085784f6c1206471e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating for Evaluation (BART):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calculating Scores for Question Generation Model (BART) ---\n",
      "BLEU Score: 25.79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a56a2228f247fbb25742b3ce914541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8e90e93ddc466f986ace0769e37725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46369254b2b4e21b61e6ece32c1695d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b99c707481a448d85b2bb2100c8852b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e942b677dae4417e909ee03431221def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac369b093b0414d8586cc72d5699232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore (F1): 0.9193\n",
      "\n",
      "--- Calculating Scores for Answer Generation Model (BART) ---\n",
      "BLEU Score: 12.71\n",
      "BERTScore (F1): 0.8796\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\\n--- Step 5: Evaluating BART Model Performance ---\")\n",
    "\n",
    "# Make sure the libraries are installed\n",
    "!pip install evaluate sacrebleu bert-score tqdm -q\n",
    "\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Load the metric calculators\n",
    "try:\n",
    "    bleu_metric = evaluate.load('sacrebleu')\n",
    "    bertscore_metric = evaluate.load('bertscore')\n",
    "    print(\"Evaluation metrics loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load metrics: {e}\")\n",
    "\n",
    "# Let's evaluate on a random sample of 100 items\n",
    "EVAL_SAMPLE_SIZE = 100\n",
    "if len(df) > EVAL_SAMPLE_SIZE:\n",
    "    eval_df = df.sample(n=EVAL_SAMPLE_SIZE, random_state=42)\n",
    "else:\n",
    "    eval_df = df\n",
    "\n",
    "print(f\"Running evaluation on {len(eval_df)} samples...\")\n",
    "\n",
    "# Store the generated texts and the reference texts\n",
    "generated_questions = []\n",
    "reference_questions = []\n",
    "generated_answers = []\n",
    "reference_answers = []\n",
    "\n",
    "# Loop through the evaluation sample and generate predictions\n",
    "# This loop is IDENTICAL to the T5 one. It works because our\n",
    "# `generate_question` and `generate_answer` functions are now\n",
    "# powered by the BART models you just loaded.\n",
    "for _, row in tqdm(eval_df.iterrows(), total=len(eval_df), desc=\"Generating for Evaluation (BART)\"):\n",
    "    # --- Part 1: Evaluate the Question Generator ---\n",
    "    ref_q = row['Question']\n",
    "    reference_questions.append(ref_q)\n",
    "    \n",
    "    gen_q = generate_question(\n",
    "        explanation=row['Explanation'],\n",
    "        grade=row['grade'],\n",
    "        subject=row['subject'],\n",
    "        difficulty=row['Difficulty'],\n",
    "        complexity=row['QuestionComplexity']\n",
    "    )\n",
    "    generated_questions.append(gen_q)\n",
    "\n",
    "    # --- Part 2: Evaluate the Answer Generator ---\n",
    "    ref_a = row['Answer']\n",
    "    reference_answers.append(ref_a)\n",
    "    \n",
    "    gen_a = generate_answer(\n",
    "        explanation=row['Explanation'],\n",
    "        grade=row['grade'],\n",
    "        subject=row['subject'],\n",
    "        difficulty=row['Difficulty'],\n",
    "        question=gen_q # Use the generated question\n",
    "    )\n",
    "    generated_answers.append(gen_a)\n",
    "\n",
    "# --- Now, calculate the scores ---\n",
    "\n",
    "bleu_references_q = [[q] for q in reference_questions]\n",
    "bleu_references_a = [[a] for a in reference_answers]\n",
    "\n",
    "print(\"\\n--- Calculating Scores for Question Generation Model (BART) ---\")\n",
    "try:\n",
    "    bleu_score_q = bleu_metric.compute(predictions=generated_questions, references=bleu_references_q)\n",
    "    print(f\"BLEU Score: {bleu_score_q['score']:.2f}\")\n",
    "\n",
    "    bert_score_q = bertscore_metric.compute(predictions=generated_questions, references=reference_questions, lang=\"en\")\n",
    "    avg_f1_q = np.mean(bert_score_q['f1'])\n",
    "    print(f\"BERTScore (F1): {avg_f1_q:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during question scoring: {e}\")\n",
    "\n",
    "print(\"\\n--- Calculating Scores for Answer Generation Model (BART) ---\")\n",
    "try:\n",
    "    bleu_score_a = bleu_metric.compute(predictions=generated_answers, references=bleu_references_a)\n",
    "    print(f\"BLEU Score: {bleu_score_a['score']:.2f}\")\n",
    "\n",
    "    bert_score_a = bertscore_metric.compute(predictions=generated_answers, references=reference_answers, lang=\"en\")\n",
    "    avg_f1_a = np.mean(bert_score_a['f1'])\n",
    "    print(f\"BERTScore (F1): {avg_f1_a:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during answer scoring: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-SQG Implementation for Question Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas torch transformers datasets scikit-learn sentencepiece accelerate tqdm -q\n",
    "\n",
    "\n",
    "class BERT_SQG(nn.Module):\n",
    "    def __init__(self, bert_model_name='bert-base-uncased'):\n",
    "        super().__init__()\n",
    "        # Load the BERT configuration\n",
    "        self.config = BertConfig.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Load the base BERT model (the Encoder)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Add the classification head on top\n",
    "        # This head predicts a word from the vocab for a given hidden state\n",
    "        self.cls = BertOnlyMLMHead(self.config)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Pass the input through the base BERT model\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Get the hidden state of all tokens\n",
    "        sequence_output = outputs[0]\n",
    "        \n",
    "        # Pass the hidden states to the MLM head to get logits for each token\n",
    "        prediction_scores = self.cls(sequence_output)\n",
    "        \n",
    "        return prediction_scores\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Instantiate the model and resize token embeddings for the new [HL] token\n",
    "model = BERT_SQG()\n",
    "model.bert.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model defined and loaded.\")\n",
    "\n",
    "\n",
    "def create_training_examples(row):\n",
    "    context = row['Explanation']\n",
    "    answer = row['Answer']\n",
    "    question = row['Question']\n",
    "    \n",
    "        \n",
    "    prompt = f\"[CLS] {context} [SEP] {answer} [SEP]\"\n",
    "    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    \n",
    "    question_tokens = tokenizer.encode(question, add_special_tokens=False)\n",
    "    question_tokens.append(tokenizer.sep_token_id) # Add [SEP] as end token\n",
    "\n",
    "    examples = []\n",
    "    \n",
    "    for i in range(len(question_tokens)):\n",
    "        generated_part = question_tokens[:i] \n",
    "        input_tokens = prompt_tokens + generated_part + [tokenizer.mask_token_id]\n",
    "        \n",
    "        # 2. Create the label\n",
    "        # The label is the token we are trying to predict\n",
    "        label_token = question_tokens[i]\n",
    "        \n",
    "        # Truncate to BERT's max length\n",
    "        input_tokens = input_tokens[:511] # Leave one spot for the [MASK]\n",
    "        \n",
    "        # If we truncated the prompt, we can't create examples\n",
    "        if len(prompt_tokens) >= 511:\n",
    "            break\n",
    "            \n",
    "        # Ensure [MASK] is at the end if truncated\n",
    "        if input_tokens[-1] != tokenizer.mask_token_id:\n",
    "            input_tokens[-1] = tokenizer.mask_token_id\n",
    "            \n",
    "        examples.append({\n",
    "            'input_ids': input_tokens,\n",
    "            'label_id': label_token\n",
    "        })\n",
    "        \n",
    "        # Stop if we've already predicted the [SEP] token\n",
    "        if label_token == tokenizer.sep_token_id:\n",
    "            break\n",
    "            \n",
    "    return examples\n",
    "\n",
    "all_training_examples = []\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    all_training_examples.extend(create_training_examples(row))\n",
    "\n",
    "print(f\"Created {len(all_training_examples)} training examples from {len(df)} rows.\")\n",
    "\n",
    "# Create a custom PyTorch Dataset\n",
    "class SQGDataset(Dataset):\n",
    "    def __init__(self, examples, tokenizer, max_len=512):\n",
    "        self.examples = examples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        input_ids = example['input_ids']\n",
    "        label_id = example['label_id']\n",
    "        \n",
    "        # Pad the input_ids\n",
    "        padding_length = self.max_len - len(input_ids)\n",
    "        attention_mask = [1] * len(input_ids) + [0] * padding_length\n",
    "        input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n",
    "        \n",
    "        # Find the index of the [MASK] token\n",
    "        # This is where we will get the logits from\n",
    "        try:\n",
    "            mask_index = input_ids.index(self.tokenizer.mask_token_id)\n",
    "        except ValueError:\n",
    "            # Should not happen, but as a fallback\n",
    "            mask_index = len(example['input_ids']) - 1 \n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'mask_index': torch.tensor(mask_index, dtype=torch.long),\n",
    "            'label_id': torch.tensor(label_id, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = SQGDataset(all_training_examples, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True) # Adjust batch_size based on VRAM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5) # [cite: 285]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "NUM_EPOCHS = 1 \n",
    "\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    \n",
    "    for batch in loop:\n",
    "        # Send data to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        mask_indices = batch['mask_index'].to(device)\n",
    "        labels = batch['label_id'].to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(input_ids, attention_mask)\n",
    "\n",
    "        # Get the logits for *only* the [MASK] token\n",
    "        # This is the core logic from the paper [cite: 141-144]\n",
    "        # We need to gather the logits from the correct index for each item in the batch\n",
    "        mask_logits = logits.gather(1, mask_indices.view(-1, 1, 1).expand(-1, -1, logits.size(-1))).squeeze(1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(mask_logits, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Save the model\n",
    "model.bert.save_pretrained(\"./bert_sqg_model\")\n",
    "tokenizer.save_pretrained(\"./bert_sqg_model\")\n",
    "torch.save(model.cls.state_dict(), \"./bert_sqg_model/cls_head.pt\")\n",
    "print(\"Model saved to ./bert_sqg_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n--- Running Inference with BERT-SQG ---\")\n",
    "\n",
    "# Load the model back (for demonstration)\n",
    "model = BERT_SQG()\n",
    "model.bert = BertModel.from_pretrained(\"./bert_sqg_model\")\n",
    "model.cls.load_state_dict(torch.load(\"./bert_sqg_model/cls_head.pt\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./bert_sqg_model\")\n",
    "\n",
    "def generate_question_bert(context, answer, max_gen_len=20):\n",
    "    \"\"\"\n",
    "    Generates a question token-by-token, as shown in\n",
    "    Table 1 and 2 of the paper[cite: 158, 259].\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Format the prompt\n",
    "        \n",
    "    prompt = f\"[CLS] {context} [SEP] {answer} [SEP]\"\n",
    "    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    \n",
    "    # Start with the [MASK] token\n",
    "    generated_tokens = []\n",
    "    input_tokens = prompt_tokens + [tokenizer.mask_token_id]\n",
    "    \n",
    "    for _ in range(max_gen_len):\n",
    "        # Convert to tensor\n",
    "        input_ids = torch.tensor([input_tokens]).to(device)\n",
    "        # Create a simple attention mask\n",
    "        attention_mask = torch.ones_like(input_ids).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Get the logits for the last token (the [MASK] token)\n",
    "        # This is the core of the sequential generation [cite: 141]\n",
    "        next_token_logits = logits[0, -1, :]\n",
    "        \n",
    "        # Get the predicted token ID\n",
    "        predicted_token_id = torch.argmax(next_token_logits).item()\n",
    "        \n",
    "        # If it's the [SEP] token, we're done\n",
    "        if predicted_token_id == tokenizer.sep_token_id:\n",
    "            break\n",
    "            \n",
    "        # Add the new token to our generated list\n",
    "        generated_tokens.append(predicted_token_id)\n",
    "        \n",
    "        # Prepare the input for the next loop:\n",
    "        # [CLS]...[SEP]...[SEP] + generated_tokens + [MASK]\n",
    "        input_tokens = prompt_tokens + generated_tokens + [tokenizer.mask_token_id]\n",
    "        \n",
    "        # Stop if we exceed max length\n",
    "        if len(input_tokens) >= 512:\n",
    "            break\n",
    "            \n",
    "    # Decode the generated tokens\n",
    "    return tokenizer.decode(generated_tokens)\n",
    "\n",
    "\n",
    "# --- Let's test it with a sample ---\n",
    "sample_row = df.iloc[5]\n",
    "explanation = sample_row['Explanation']\n",
    "answer = sample_row['Answer']\n",
    "original_question = sample_row['Question']\n",
    "\n",
    "print(\"\\n--- Generating a new question (BERT-SQG): ---\")\n",
    "print(f\"CONTEXT: {explanation[:200]}...\")\n",
    "print(f\"ANSWER: {answer}\")\n",
    "\n",
    "generated_q_bert = generate_question_bert(explanation, answer)\n",
    "\n",
    "print(\"\\nGENERATED QUESTION (BERT-SQG):\")\n",
    "print(generated_q_bert)\n",
    "print(\"\\nORIGINAL QUESTION (for comparison):\")\n",
    "print(original_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-13T22:23:45.149095Z",
     "iopub.status.idle": "2025-11-13T22:23:45.149476Z",
     "shell.execute_reply": "2025-11-13T22:23:45.149348Z",
     "shell.execute_reply.started": "2025-11-13T22:23:45.149334Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\n--- Step 6: Evaluating BERT-SQG Model Performance ---\")\n",
    "\n",
    "!pip install evaluate sacrebleu bert-score tqdm -q\n",
    "\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Load the metric calculators\n",
    "try:\n",
    "    bleu_metric = evaluate.load('sacrebleu')\n",
    "    bertscore_metric = evaluate.load('bertscore')\n",
    "    print(\"Evaluation metrics loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load metrics: {e}\")\n",
    "\n",
    "# Let's evaluate on a random sample of 100 items from our original filtered dataframe.\n",
    "# This keeps the comparison fair, as we used 100 for T5 and BART.\n",
    "EVAL_SAMPLE_SIZE = 100\n",
    "if len(df) > EVAL_SAMPLE_SIZE:\n",
    "    # Use the main 'df' to get a good evaluation sample\n",
    "    eval_df = df.sample(n=EVAL_SAMPLE_SIZE, random_state=42)\n",
    "else:\n",
    "    eval_df = df\n",
    "\n",
    "print(f\"Running evaluation on {len(eval_df)} samples...\")\n",
    "\n",
    "# Store the generated texts and the reference texts\n",
    "generated_questions = []\n",
    "reference_questions = []\n",
    "\n",
    "# Loop through the evaluation sample and generate predictions\n",
    "# We will use our custom generate_question_bert function\n",
    "for _, row in tqdm(eval_df.iterrows(), total=len(eval_df), desc=\"Generating for Evaluation (BERT-SQG)\"):\n",
    "    # Get the reference (original) question\n",
    "    ref_q = row['Question']\n",
    "    reference_questions.append(ref_q)\n",
    "    \n",
    "    # Generate a new question from the context using our custom function\n",
    "    gen_q = generate_question_bert(\n",
    "        context=row['Explanation'],\n",
    "        answer=row['Answer']\n",
    "    )\n",
    "    generated_questions.append(gen_q)\n",
    "\n",
    "\n",
    "# --- Now, calculate the scores ---\n",
    "\n",
    "# For BLEU, the references need to be in a list of lists.\n",
    "bleu_references_q = [[q] for q in reference_questions]\n",
    "\n",
    "print(\"\\n--- Calculating Scores for Question Generation Model (BERT-SQG) ---\")\n",
    "try:\n",
    "    # Calculate BLEU Score for Questions\n",
    "    bleu_score_q = bleu_metric.compute(predictions=generated_questions, references=bleu_references_q)\n",
    "    print(f\"BLEU Score: {bleu_score_q['score']:.2f}\")\n",
    "\n",
    "    # Calculate BERTScore for Questions\n",
    "    bert_score_q = bertscore_metric.compute(predictions=generated_questions, references=reference_questions, lang=\"en\")\n",
    "    # We take the average F1 score\n",
    "    avg_f1_q = np.mean(bert_score_q['f1'])\n",
    "    print(f\"BERTScore (F1): {avg_f1_q:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during question scoring: {e}\")\n",
    "\n",
    "print(\"\\n--- Model Comparison ---\")\n",
    "print(\"You now have the BLEU and BERTScore for your BERT-SQG baseline.\")\n",
    "print(\"Compare these scores to the ones you got for T5 and BART.\")\n",
    "print(f\"As the paper found [cite: D19-5821.pdf], this 'BERT-SQG' should be much better than a naive BERT,\")\n",
    "print(\"but you can now see how it compares to more modern architectures like T5 and BART!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
